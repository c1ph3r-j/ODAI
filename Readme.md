# ODAI (On Device AI) - LLM On-Device Inference with ONNX Runtime in Android Java

This sample project demonstrates how to run a Language Model (LLM) on an Android device using the ONNX Runtime with the LLaMA 3.2 1B model. The project showcases the capabilities of ONNX to perform efficient inference on mobile devices.

give a star if you like it.

## Features

- **ONNX Runtime**: Utilize ONNX for efficient model inference on Android.
- **LLaMA 3.2 1B**: Leverage the powerful LLaMA model for language tasks.
- **Cross-Platform**: This setup is also available in Flutter and iOS.

## Getting Started

### Prerequisites

- Android Studio
- Android SDK
- Java 17
- ONNX Runtime for Android
- **Recommended**: A real device with more than 8GB of RAM for optimal performance.

### Installation

1. Clone this repository:

   ```bash
   git clone https://github.com/c1ph3r-j/ODAI.git
   ```

2. Open the project in Android Studio.

3. Run the project on a real device with sufficient RAM.

### Sample Screenshot (Samsung S20 Fe 5G - snapdragon)

![Sample Screenshot](images/sample_image_1.png)

![Sample Screenshot](images/sample_image_2.png)

## Additional Resources

For more information about ONNX Runtime and additional examples, check the official Microsoft repository:

- [ONNX Runtime Inference Examples](https://github.com/microsoft/onnxruntime-inference-examples.git)


## Acknowledgments

- Thanks to the ONNX team for their incredible work on ONNX Runtime.
- Special thanks to the developers of the LLaMA model for providing such a powerful tool for language tasks.
